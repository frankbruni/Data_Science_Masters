{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Publish and Consume Messages with Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spin up Cluster\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "#### Make sure it is Up\n",
    "```\n",
    "docker-compose ps\n",
    "```\n",
    "#### Start Kafka Logs\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```\n",
    "#### Create Topic\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "#### Check out the Topic\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "```\n",
    "#### Check out Messagess\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json\"\n",
    "```\n",
    "    \n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json | jq '.'\"\n",
    "```\n",
    "    \n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c\"\n",
    "```\n",
    "#### Publish Test Messages to Topic Assessments\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Produced 100 messages.'\"\n",
    "```\n",
    "#### Consume Messages\n",
    "```\n",
    "docker-compose exec kafka kafka-console-consumer --bootstrap-server kafka:29092 --topic assessments --from-beginning --max-messages 42\n",
    "```\n",
    "#### Tear Down Cluster\n",
    "```\n",
    "docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Use Spark to Transform the Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spin up Cluster\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "#### Make sure it is Up\n",
    "```\n",
    "docker-compose ps\n",
    "```\n",
    "#### Start Kafka Logs\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```\n",
    "#### Create Topic\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "#### Check out the Topic\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "```\n",
    "#### Check out Messagess\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json\"\n",
    "```\n",
    "    \n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json | jq '.'\"\n",
    "```\n",
    "    \n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c\"\n",
    "```\n",
    "#### Publish Test Messages to Topic Assessments\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Produced 100 messages.'\"\n",
    "```\n",
    "#### Run Spark\n",
    "```\n",
    "docker-compose exec spark pyspark\n",
    "```\n",
    "#### Read from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     4|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     5|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     6|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     7|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     8|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     9|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    10|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    11|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    12|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    13|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    14|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    15|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    16|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    17|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    18|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    19|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages_as_strings=messages.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| key|               value|\n",
      "+----+--------------------+\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_as_strings.show()\n",
    "\n",
    "messages_as_strings.printSchema()\n",
    "\n",
    "messages_as_strings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":true,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f106765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_as_strings.select('value').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":true,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f106765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_as_strings.select('value').take(1)[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_exam_id': '37f0a30a-7464-11e6-aa92-a8667f27e5dc',\n",
       " 'certification': 'false',\n",
       " 'exam_name': 'Normal Forms and All That Jazz Master Class',\n",
       " 'keen_created_at': '1516717442.735266',\n",
       " 'keen_id': '5a6745820eb8ab00016be1f1',\n",
       " 'keen_timestamp': '1516717442.735266',\n",
       " 'max_attempts': '1.0',\n",
       " 'sequences': {'attempt': 1,\n",
       "  'counts': {'all_correct': False,\n",
       "   'correct': 2,\n",
       "   'incomplete': 1,\n",
       "   'incorrect': 1,\n",
       "   'submitted': 4,\n",
       "   'total': 4,\n",
       "   'unanswered': 0},\n",
       "  'id': '5b28a462-7a3b-42e0-b508-09f3906d1703',\n",
       "  'questions': [{'id': '7a2ed6d3-f492-49b3-b8aa-d080a8aad986',\n",
       "    'options': [{'at': '2018-01-23T14:23:24.670Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '49c574b4-5c82-4ffd-9bd1-c3358faf850d',\n",
       "      'submitted': 1},\n",
       "     {'at': '2018-01-23T14:23:25.914Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': 'f2528210-35c3-4320-acf3-9056567ea19f',\n",
       "      'submitted': 1},\n",
       "     {'checked': False,\n",
       "      'correct': True,\n",
       "      'id': 'd1bf026f-554f-4543-bdd2-54dcf105b826'}],\n",
       "    'user_correct': False,\n",
       "    'user_incomplete': True,\n",
       "    'user_result': 'missed_some',\n",
       "    'user_submitted': True},\n",
       "   {'id': 'bbed4358-999d-4462-9596-bad5173a6ecb',\n",
       "    'options': [{'at': '2018-01-23T14:23:30.116Z',\n",
       "      'checked': True,\n",
       "      'id': 'a35d0e80-8c49-415d-b8cb-c21a02627e2b',\n",
       "      'submitted': 1},\n",
       "     {'checked': False,\n",
       "      'correct': True,\n",
       "      'id': 'bccd6e2e-2cef-4c72-8bfa-317db0ac48bb'},\n",
       "     {'at': '2018-01-23T14:23:41.791Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '7e0b639a-2ef8-4604-b7eb-5018bd81a91b',\n",
       "      'submitted': 1}],\n",
       "    'user_correct': False,\n",
       "    'user_incomplete': False,\n",
       "    'user_result': 'incorrect',\n",
       "    'user_submitted': True},\n",
       "   {'id': 'e6ad8644-96b1-4617-b37b-a263dded202c',\n",
       "    'options': [{'at': '2018-01-23T14:23:52.510Z',\n",
       "      'checked': False,\n",
       "      'id': 'a9333679-de9d-41ff-bb3d-b239d6b95732'},\n",
       "     {'checked': False, 'id': '85795acc-b4b1-4510-bd6e-41648a3553c9'},\n",
       "     {'at': '2018-01-23T14:23:54.223Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': 'c185ecdb-48fb-4edb-ae4e-0204ac7a0909',\n",
       "      'submitted': 1},\n",
       "     {'at': '2018-01-23T14:23:53.862Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '77a66c83-d001-45cd-9a5a-6bba8eb7389e',\n",
       "      'submitted': 1}],\n",
       "    'user_correct': True,\n",
       "    'user_incomplete': False,\n",
       "    'user_result': 'correct',\n",
       "    'user_submitted': True},\n",
       "   {'id': '95194331-ac43-454e-83de-ea8913067055',\n",
       "    'options': [{'checked': False,\n",
       "      'id': '59b9fc4b-f239-4850-b1f9-912d1fd3ca13'},\n",
       "     {'checked': False, 'id': '2c29e8e8-d4a8-406e-9cdf-de28ec5890fe'},\n",
       "     {'checked': False, 'id': '62feee6e-9b76-4123-bd9e-c0b35126b1f1'},\n",
       "     {'at': '2018-01-23T14:24:00.807Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '7f13df9c-fcbe-4424-914f-2206f106765c',\n",
       "      'submitted': 1}],\n",
       "    'user_correct': True,\n",
       "    'user_incomplete': False,\n",
       "    'user_result': 'correct',\n",
       "    'user_submitted': True}]},\n",
       " 'started_at': '2018-01-23T14:23:19.082Z',\n",
       " 'user_exam_id': '6d4089e4-bde5-4a22-b65f-18bce9ab79c8'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "first_message=json.loads(messages_as_strings.select('value').take(1)[0].value)\n",
    "\n",
    "first_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Pyspark\n",
    "    `exit()`\n",
    "#### Tear Down Cluster\n",
    "    `docker-compose down`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Use Spark to Transform the Messages and Land them in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spin up Cluster\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "#### Make sure it is Up\n",
    "```\n",
    "docker-compose ps\n",
    "```\n",
    "#### Start Kafka Logs\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```\n",
    "#### Create Topic\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "#### Check out the Topic\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "```\n",
    "#### Publish to Kafka topic assessments\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-frankbruni/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```\n",
    "#### Exec a Bash Shell into Spark Container\n",
    "```\n",
    "docker-compose exec spark bash\n",
    "```\n",
    "#### Symbolic Link from Spark to W205 Directory\n",
    "```\n",
    "ln -s /w205 w205\n",
    "```\n",
    "#### Exit Container\n",
    "```\n",
    "exit\n",
    "```\n",
    "#### Start Jupyter Notebook with Pyspark Kernel\n",
    "```\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Json Data and Answer:\n",
    " - What were the top 3 <b>most</b> taken course?\n",
    " - What were the top 3 <b>least</b> taken courses?\n",
    " - What is the <b>average score</b> on each test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "extracted_assessments.registerTempTable('assessments')\n",
    "\n",
    "extracted_assessments.write.mode('overwrite').parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             keen_id|\n",
      "+--------------------+\n",
      "|5a6745820eb8ab000...|\n",
      "|5a674541ab6b0a000...|\n",
      "|5a67999d3ed3e3000...|\n",
      "|5a6799694fc7c7000...|\n",
      "|5a6791e824fccd000...|\n",
      "|5a67a0b6852c2a000...|\n",
      "|5a67b627cc80e6000...|\n",
      "|5a67ac8cb0a5f4000...|\n",
      "|5a67a9ba060087000...|\n",
      "|5a67ac54411aed000...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select keen_id from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data imported we can begin unrolling the json data. I'll first take a look at the exam names and explore how often each exam was taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_sequences_id(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_dict = {\"base_exam_id\" : raw_dict[\"base_exam_id\"],\n",
    "               \"exam_name\" : raw_dict[\"exam_name\"],\n",
    "               \"keen_id\" : raw_dict[\"keen_id\"]}\n",
    "    return Row(**my_dict)\n",
    "\n",
    "my_sequences = assessments.rdd.map(my_lambda_sequences_id).toDF()\n",
    "\n",
    "my_sequences.registerTempTable('sequences')\n",
    "\n",
    "my_sequences.write.mode('overwrite').parquet(\"/tmp/my_sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|           exam_name|             keen_id|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|Normal Forms and ...|5a6745820eb8ab000...|\n",
      "|37f0a30a-7464-11e...|Normal Forms and ...|5a674541ab6b0a000...|\n",
      "|4beeac16-bb83-4d5...|The Principles of...|5a67999d3ed3e3000...|\n",
      "|4beeac16-bb83-4d5...|The Principles of...|5a6799694fc7c7000...|\n",
      "|6442707e-7488-11e...|Introduction to B...|5a6791e824fccd000...|\n",
      "|8b4488de-43a5-4ff...|        Learning Git|5a67a0b6852c2a000...|\n",
      "|e1f07fac-5566-4fd...|Git Fundamentals ...|5a67b627cc80e6000...|\n",
      "|7e2e0b53-a7ba-458...|Introduction to P...|5a67ac8cb0a5f4000...|\n",
      "|1a233da8-e6e5-48a...|Intermediate Pyth...|5a67a9ba060087000...|\n",
      "|7e2e0b53-a7ba-458...|Introduction to P...|5a67ac54411aed000...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from sequences limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           exam_name|num|\n",
      "+--------------------+---+\n",
      "|        Learning Git|394|\n",
      "|Introduction to P...|162|\n",
      "|Introduction to J...|158|\n",
      "|Intermediate Pyth...|158|\n",
      "|Learning to Progr...|128|\n",
      "|Introduction to M...|119|\n",
      "|Software Architec...|109|\n",
      "|Beginning C# Prog...| 95|\n",
      "|    Learning Eclipse| 85|\n",
      "|Learning Apache M...| 80|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name , count(*) as num from sequences group by exam_name order by num desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Git was taken 394 times, Introduction to Python was taken 162 times, and Introduction to Java was taken 158 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           exam_name|num|\n",
      "+--------------------+---+\n",
      "|Nulls, Three-valu...|  1|\n",
      "|Native Web Apps f...|  1|\n",
      "|Learning to Visua...|  1|\n",
      "|Operating Red Hat...|  1|\n",
      "|Client-Side Data ...|  2|\n",
      "|Hibernate and JPA...|  2|\n",
      "|Learning Spring P...|  2|\n",
      "|Arduino Prototypi...|  2|\n",
      "|The Closed World ...|  2|\n",
      "|Understanding the...|  2|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name , count(*) as num from sequences group by exam_name order by num limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least taken classes were Nulls, Native Web Apps, Learning to Visualize, and Operating Red Hat all with only one time taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_correct_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "                          \"score\" : raw_dict[\"sequences\"][\"counts\"][\"correct\"]/raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "                          \"keen_id\" : raw_dict[\"keen_id\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+-----+\n",
      "|             keen_id|correct|total|score|\n",
      "+--------------------+-------+-----+-----+\n",
      "|5a6745820eb8ab000...|      2|    4|  0.5|\n",
      "|5a674541ab6b0a000...|      1|    4| 0.25|\n",
      "|5a67999d3ed3e3000...|      3|    4| 0.75|\n",
      "|5a6799694fc7c7000...|      2|    4|  0.5|\n",
      "|5a6791e824fccd000...|      3|    4| 0.75|\n",
      "|5a67a0b6852c2a000...|      5|    5|  1.0|\n",
      "|5a67b627cc80e6000...|      1|    1|  1.0|\n",
      "|5a67ac8cb0a5f4000...|      5|    5|  1.0|\n",
      "|5a67a9ba060087000...|      4|    4|  1.0|\n",
      "|5a67ac54411aed000...|      0|    5|  0.0|\n",
      "+--------------------+-------+-----+-----+\n",
      "\n",
      "+-----------------+\n",
      "|        avg_score|\n",
      "+-----------------+\n",
      "|62.65699745547047|\n",
      "+-----------------+\n",
      "\n",
      "+-------------------+\n",
      "| standard_deviation|\n",
      "+-------------------+\n",
      "|0.31086692286170553|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "\n",
    "my_correct_total.registerTempTable('ct')\n",
    "\n",
    "my_correct_total.write.mode('overwrite').parquet(\"/tmp/my_correct_total\")\n",
    "\n",
    "spark.sql(\"select keen_id, correct, total, score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE test_score AS select a.exam_name , b.score from sequences as a,ct as b where a.keen_id=b.keen_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+---+\n",
      "|           exam_name|         avg_score|num|\n",
      "+--------------------+------------------+---+\n",
      "|        Learning Git|0.6827586206896553|406|\n",
      "|Introduction to P...|0.5666666666666665|162|\n",
      "|Intermediate Pyth...|0.5092592592592593|162|\n",
      "|Introduction to J...|0.8759493670886075|158|\n",
      "|Beginning C# Prog...|0.5629770992366412|131|\n",
      "|Learning to Progr...| 0.544642857142857|128|\n",
      "|Introduction to M...|0.6869747899159664|119|\n",
      "|Software Architec...|0.4793577981651376|109|\n",
      "|    Learning Eclipse|0.7058823529411765| 85|\n",
      "|Introduction to B...|0.6450617283950617| 81|\n",
      "+--------------------+------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name , AVG(score) as avg_score, count(*) as num from test_score group by exam_name order by num desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the average score for each test ordered by the most popular test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
