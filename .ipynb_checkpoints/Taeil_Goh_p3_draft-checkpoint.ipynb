{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4r1L69aGoZG0"
   },
   "source": [
    "# Project 3: Poisonous Mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btVARsAroZG1"
   },
   "source": [
    "In this project, you'll investigate properties of mushrooms. This classic dataset contains over 8000 observations, where each mushroom is described by a variety of features like color, odor, etc., and the target variable is an indicator for whether the mushroom is poisonous. Since all the observations are categorical, I've binarized the feature space. Look at the feature_names below to see all 126 binary names.\n",
    "\n",
    "You'll start by running PCA to reduce the dimensionality from 126 down to 2 so that you can easily visualize the data. In general, PCA is very useful for visualization (though sklearn.manifold.tsne is known to produce better visualizations). Recall that PCA is a linear transformation. The 1st projected dimension is the linear combination of all 126 original features that captures as much of the variance in the data as possible. The 2nd projected dimension is the linear combination of all 126 original features that captures as much of the remaining variance as possible. The idea of dense low dimensional representations is crucial to machine learning!\n",
    "\n",
    "Once you've projected the data to 2 dimensions, you'll experiment with clustering using KMeans and density estimation with Gaussian Mixture Models. Finally, you'll train a classifier by fitting a GMM for the positive class and a GMM for the negative class, and perform inference by comparing the probabilities output by each model.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDvuh15loZG2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import urllib.request as urllib2 # For python3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKyn8WO1pJwK"
   },
   "outputs": [],
   "source": [
    "MUSHROOM_DATA = 'https://raw.githubusercontent.com/UCB-MIDS/207-Applied-Machine-Learning/master/Data/mushroom.data'\n",
    "MUSHROOM_MAP = 'https://raw.githubusercontent.com/UCB-MIDS/207-Applied-Machine-Learning/master/Data/mushroom.map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28FIvBtVoZG4"
   },
   "source": [
    "Load feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXUv_S1YoZG4"
   },
   "outputs": [],
   "source": [
    "feature_names = []\n",
    "\n",
    "for line in urllib2.urlopen(MUSHROOM_MAP):\n",
    "    [index, name, junk] = line.decode('utf-8').split()\n",
    "    feature_names.append(name)\n",
    "\n",
    "print('Loaded feature names: ', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8u0iHqPPl8A"
   },
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k034gVFboZHA"
   },
   "source": [
    "Load data. The data is sparse in the input file, but there aren't too many features, so we'll use a dense representation, which is supported by all sklearn objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [
     2
    ],
    "colab": {},
    "colab_type": "code",
    "id": "gg6IVy0loZHA"
   },
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for line in urllib2.urlopen(MUSHROOM_DATA):\n",
    "    items = line.decode('utf-8').split()\n",
    "    Y.append(int(items.pop(0)))\n",
    "    x = np.zeros(len(feature_names))\n",
    "    for item in items:\n",
    "        feature = int(str(item).split(':')[0])\n",
    "        x[feature] = 1\n",
    "    X.append(x)\n",
    "\n",
    "# Convert these lists to numpy arrays.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Split into train and test data.\n",
    "train_data, train_labels = X[:7000], Y[:7000]\n",
    "test_data, test_labels = X[7000:], Y[7000:]\n",
    "\n",
    "# Check that the shapes look right.\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nr25XV7BoZHC"
   },
   "source": [
    "### Part 1:\n",
    "\n",
    "Run Principal Components Analysis on the data. Show what fraction of the total variance in the training data is explained by the first k principal components, for k in [1, 50].\n",
    "\n",
    "#### different section \n",
    "Do a principal components analysis on the data. Show what fraction of the total variance in the training data is explained by the first k principal components, for k in [1, 2, 3, 4, 5, 10, 20, 30, 40, 50].  Also show a lineplot of fraction of total variance vs. number of principal components, for all possible numbers of principal components.\n",
    "\n",
    "Notes:\n",
    "* You can use `PCA` to produce a PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "frHUWcZUoZHC"
   },
   "outputs": [],
   "source": [
    "def P1():\n",
    "### STUDENT START ###\n",
    "\n",
    "    pca_mod = PCA(n_components = 50, random_state=12345)\n",
    "    pca_mod.fit(train_data)\n",
    "\n",
    "    var_explained = pca_mod.explained_variance_ratio_.cumsum()\n",
    "\n",
    "    #print('Explained variance ratio: \\n', pca_mod.explained_variance_ratio_)\n",
    "    print('Cumulative explained variance: \\n', var_explained)\n",
    "\n",
    "    #print('PCA components: \\n', pca_mod.components_)\n",
    "\n",
    "    plt.plot(range(1, 51), var_explained, label=\"variance\")\n",
    "    plt.scatter(range(1, 51), var_explained, label=\"variance\")\n",
    "    plt.title(\"Cumulative Explained Variance vs k\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"the first k principal components\")\n",
    "    plt.ylabel(\"the explained variance\")\n",
    "    plt.show()\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbGl1D3RoZHE"
   },
   "source": [
    "### Part 2:\n",
    "\n",
    "PCA can be very useful for visualizing data. Project the training data down to 2 dimensions and plot it. Show the positive (poisonous) cases in blue and the negative (non-poisonous) in red. Here's a reference for plotting: http://matplotlib.org/users/pyplot_tutorial.html\n",
    "\n",
    "#### different secttion \n",
    "PCA can be very useful for visualizing data. Project the training data down to 2 dimensions and show as a square scatterplot. Show the positive (poisonous) examples in red and the negative (non-poisonous) examples in green. Here's a reference for plotting: http://matplotlib.org/users/pyplot_tutorial.html\n",
    "\n",
    "Notes:\n",
    "* You can use `PCA` to produce a PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "BFj_rQ7OPl8M"
   },
   "outputs": [],
   "source": [
    "def P2():\n",
    "### STUDENT START ###\n",
    "    pca_mod = PCA(n_components = 2, random_state=12345)\n",
    "    X_train_2d = pca_mod.fit_transform(train_data)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    #plot positive examples\n",
    "    plt.scatter(X_train_2d[train_labels == 1][:, 0], X_train_2d[train_labels == 1][:, 1], \n",
    "                c=\"b\", s=1, label=\"positive (poisonous)\")\n",
    "\n",
    "    #plot negative examples\n",
    "    plt.scatter(X_train_2d[train_labels == 0][:, 0], X_train_2d[train_labels == 0][:, 1], \n",
    "                c=\"r\", s=1, label=\"negative (non-poisonous)\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"PCA component 1\")\n",
    "    plt.ylabel(\"PCA component 2\")\n",
    "    plt.title(\"PCA 2d projected\")\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yuHTFCyNoZHG"
   },
   "source": [
    "### Part 3:\n",
    "\n",
    "- Run KMeans with [1,16] clusters over the 2d projected data. \n",
    "- Mark each centroid cluster and plot a circle that goes through the most distant point assigned to each cluster.\n",
    "\n",
    "#### different section \n",
    "- Fit a k-means cluster model with 6 clusters over the 2d projected data. \n",
    "- As in part 2, show as a square scatterplot with the positive (poisonous) examples in red and the negative (non-poisonous) examples in green.  \n",
    "- For each cluster, mark the centroid and plot a circle that goes through the cluster's example that is most distant from the centroid.\n",
    "\n",
    "Notes:\n",
    "* You can use `KMeans` to produce a k-means cluster analysis.\n",
    "* You can use `linalg.norm` to determine distance (dissimilarity) between observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "gJcVJnkPoZHG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def P3():\n",
    "### STUDENT START ###\n",
    "\n",
    "    pca_mod = PCA(n_components = 2, random_state=12345)\n",
    "    X_train_2d = pca_mod.fit_transform(train_data)\n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "\n",
    "    for n in range(1, 17):\n",
    "        km = KMeans (n_clusters=n, init='k-means++')\n",
    "        clstrs = km.fit (X_train_2d)\n",
    "        p = plt.subplot(4, 4, n)\n",
    "        \n",
    "        #color by lables\n",
    "        plt.scatter(X_train_2d[train_labels == 1][:, 0], X_train_2d[train_labels == 1][:, 1], c=\"blue\", s=1)\n",
    "        plt.scatter(X_train_2d[train_labels == 0][:, 0], X_train_2d[train_labels == 0][:, 1], c=\"red\", s=1)\n",
    "\n",
    "        centroids = clstrs.cluster_centers_\n",
    "\n",
    "        #plot centroids\n",
    "        p.scatter(centroids[:,0], centroids[:,1], c=\"green\", marker=\"+\", s=100)\n",
    "\n",
    "        #calculate furthest using L2 distance from centroids and\n",
    "        for i in range(n):\n",
    "            distance = np.linalg.norm(X_train_2d[km.labels_ == i] - centroids[i], axis=1)\n",
    "\n",
    "            #draw circle with max distance\n",
    "            circle1 = plt.Circle(centroids[i], np.max(distance), facecolor='none', edgecolor='black')\n",
    "            fig = plt.gcf()\n",
    "            ax = fig.gca()\n",
    "            ax.add_artist(circle1) \n",
    "\n",
    "        plt.title(str(n)+\" clusters\") \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v4dXc_VloZHI"
   },
   "source": [
    "### Part 4:\n",
    "\n",
    "Fit a Gaussian Mixture Model for the positive examples in your 2d projected data. Plot the estimated density contours as shown here: http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#example-mixture-plot-gmm-pdf-py. Vary the number of mixture components from 1-4 and the covariance matrix type ('spherical', 'diag', 'tied', 'full').\n",
    "\n",
    "#### different section\n",
    "\n",
    "Fit Gaussian mixture models for the positive (poisonous) examples in your 2d projected data. Vary the number of mixture components from 1 to 4 and the covariance matrix type 'spherical', 'diag', 'tied', 'full' (that's 16 models).  Show square plots of the estimated density contours presented in a 4x4 grid - one row each for a number of mixture components and one column each for a convariance matrix type.  \n",
    "\n",
    "Notes:\n",
    "* You can use `GaussianMixture(n_components=..., covariance_type=..., random_state=12345)` to produce a Gaussian mixture model.\n",
    "* You can use `contour` in combination with other methods to plot contours, like in this example: http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#example-mixture-plot-gmm-pdf-py\n",
    "* You can use `contour` without the `norm` and `levels` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24Ux-0jtPl8U"
   },
   "outputs": [],
   "source": [
    "#def P4():\n",
    "### STUDENT START ###\n",
    "\n",
    "pca_mod = PCA(n_components = 2, random_state=12345)\n",
    "X_train_2d = pca_mod.fit_transform(train_data[train_labels==1])\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.title('Negative likelihood predicted by a GMM')\n",
    "\n",
    "cov_mat_tp = ['spherical', 'diag', 'tied', 'full']\n",
    "#cov_mat_tp = ['tied']\n",
    "\n",
    "for i, cov_tp in enumerate(cov_mat_tp):\n",
    "    for n in range(1, 5):\n",
    "        gmm_mod = GaussianMixture(n_components=n, covariance_type=cov_tp, random_state=12345)\n",
    "        gmm_mod.fit(X_train_2d)\n",
    "        gmm_mod.predict(X_train_2d)\n",
    "\n",
    "        # display predicted scores by the model as a contour plot\n",
    "        x = np.linspace(-4., 4.)\n",
    "        y = np.linspace(-4., 4.)\n",
    " \n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "        Z = -gmm_mod.score_samples(XX)\n",
    "        Z = Z.reshape(X.shape)\n",
    "\n",
    "        p = plt.subplot(4, 4, n + i*4)\n",
    "        CS = p.contour(X, Y, Z)\n",
    "        CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "        \n",
    "        # no color \n",
    "        p.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=\"b\", s=0.5)\n",
    "            \n",
    "        plt.title(cov_tp + \"-\" + str(n) + \" components\")\n",
    "        plt.axis('tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMcLxdqDoZHL"
   },
   "source": [
    "### Part 5:\n",
    "\n",
    "Fit two 4-component full covariance GMMs, one for the positive examples and one for the negative examples in your 2d projected data. Predict the test examples by choosing the label for which the model gives a larger probability (use GMM.score). What is the accuracy?\n",
    "\n",
    "#### different section \n",
    "\n",
    "Fit two Gaussian mixture models, one for the positive examples and one for the negative examples in your 2d projected data. Use 4 mixture components and full convariance for each model.  Predict the test example labels by picking the labels corresponding to the larger of the two models' probabilities.  What is the accuracy of you predictions on the test data?\n",
    "\n",
    "Notes:\n",
    "* You can use `GaussianMixture(n_components=..., covariance_type=..., random_state=12345)` to produce a Gaussian mixture model.\n",
    "* You can use `GaussianMixture`'s `score_samples` method to find the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOKXtGBzoZHL"
   },
   "outputs": [],
   "source": [
    "def P5():\n",
    "### STUDENT START ###\n",
    "\n",
    "#PCA transformation \n",
    "    pca_mod = PCA(n_components = 2, random_state=12345)\n",
    "    X_train_2d = pca_mod.fit_transform(train_data)\n",
    "    X_test_2d = pca_mod.transform(test_data)\n",
    "\n",
    "    #train GMM model for positive data \n",
    "    gmm_positive = GaussianMixture(n_components=4, covariance_type='full', random_state=12345)\n",
    "    gmm_positive.fit(X_train_2d[train_labels == 1])\n",
    "\n",
    "    #train GMM model for negative data \n",
    "    gmm_negative = GaussianMixture(n_components=4, covariance_type='full', random_state=12345)\n",
    "    gmm_negative.fit(X_train_2d[train_labels == 0])\n",
    "\n",
    "    #predict the test data against two different models \n",
    "    y_hat_positive = gmm_positive.score_samples(X_test_2d)\n",
    "    y_hat_negative = gmm_negative.score_samples(X_test_2d)\n",
    "\n",
    "    #compare the probability \n",
    "    print(\"accuracy by comparing probabilities: %.4f\" \n",
    "          %(metrics.accuracy_score(test_labels, y_hat_positive > y_hat_negative)))\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWV5Ld70oZHN"
   },
   "source": [
    "### Part 6:\n",
    "\n",
    "Ideally, we'd like a model that gives the best accuracy with the fewest parameters. Run a series of experiments to find the model that gives the best accuracy with no more than 50 parameters. For example, with 3 PCA components and 2-component diagonal covariance GMMs, you'd have:\n",
    "\n",
    "( (3 mean vector + 3 covariance matrix) x 2 components ) x 2 classes = 24 parameters\n",
    "\n",
    "You should vary the number of PCA components, the number of GMM components, and the covariance type.\n",
    "\n",
    "#### different section \n",
    "\n",
    "Run a series of experiments to find the Gaussian mixture model that results in the best accuracy with no more than 50 parameters.  Do this by varying the number of PCA components, the number of GMM components, and the covariance type.\n",
    "\n",
    "Notes:\n",
    "* You can use `GaussianMixture(n_components=..., covariance_type=..., random_state=12345)` to produce a Gaussian mixture model.\n",
    "\n",
    "\n",
    "* For spherical, diag, and full covariance types:\n",
    "  * number of parameters = (number of parameters per gmm component * number of gmm components - 1) * number of classes\n",
    "  * number of parameters per gmm component includes all the means plus all the non-zero, non-duplicated values in the covariance matrix plus the mixing weight\n",
    "  * Each mixing weight parameter indicates how much to weight a particular gmm component; the -1 above accounts for the fact that the mixing weights must sum to 1, so you do not need to include the last mixing weight as its own parameter\n",
    "\n",
    "\n",
    "* To calculate the number of parameters for tied covariance type:\n",
    "  * number of parameters = (number of parameters per class - 1) * number of classes\n",
    "  * number of parameters per class includes all the means and mixing weights for all the gmm components plus all the non-zero, non-duplicated values in the one shared covariance matrix\n",
    "  * Each mixing weight parameter indicates how much to weight a particular gmm component; the -1 above accounts for the fact that the mixing weights must sum to 1, so you do not need to include the last mixing weight as its own parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWCMLsM2oZHN"
   },
   "outputs": [],
   "source": [
    "def P6():\n",
    "### STUDENT START ###\n",
    "    n_pca_all, n_gmm_all, accuracies, cov_tp_all = [], [], [], []\n",
    "\n",
    "    #PCA transformation \n",
    "    for n_pca in range(1,10):\n",
    "        pca_mod = PCA(n_components = n_pca, random_state=12345)\n",
    "        pca_X_train = pca_mod.fit_transform(train_data)\n",
    "        pca_X_test = pca_mod.transform(test_data)\n",
    "\n",
    "        #separate training data into two groups \n",
    "        X_train_positive = pca_X_train[train_labels == 1]\n",
    "        X_train_negative = pca_X_train[train_labels == 0]\n",
    "\n",
    "        cov_mat_tp = ['spherical', 'diag', 'tied', 'full']\n",
    "\n",
    "        for i, cov_tp in enumerate(cov_mat_tp):\n",
    "            for n_gmm in range(1, 10):\n",
    "                num_params = (n_pca + n_pca) * n_gmm * 2\n",
    "                if(num_params > 50):\n",
    "                    break\n",
    "\n",
    "                gmm_positive_mod = GaussianMixture(n_components=n_gmm, covariance_type=cov_tp, random_state=12345)\n",
    "                gmm_positive_mod.fit(X_train_positive)\n",
    "\n",
    "                gmm_negative_mod = GaussianMixture(n_components=n_gmm, covariance_type=cov_tp, random_state=12345)\n",
    "                gmm_negative_mod.fit(X_train_negative)\n",
    "\n",
    "                #predict the test data against two different models \n",
    "                y_hat_positive = gmm_positive_mod.score_samples(pca_X_test)\n",
    "                y_hat_negative = gmm_negative_mod.score_samples(pca_X_test)\n",
    "\n",
    "                #compare the probability\n",
    "                accuracy = metrics.accuracy_score(test_labels, y_hat_positive > y_hat_negative)\n",
    "                n_pca_all.append(n_pca)\n",
    "                n_gmm_all.append(n_gmm)\n",
    "                accuracies.append(accuracy)\n",
    "                cov_tp_all.append(cov_tp)\n",
    "\n",
    "                #number of parameters: ((# of mean vector + # of covariance matrix) x # GMM components ) x 2 classes\n",
    "                #print(\"[PCA: %d, GMM: %d components, %9s, %2d parameters] accuracy: %.4f\" \n",
    "                #      %(n_pca, n_gmm, cov_tp, num_params, accuracy))\n",
    "\n",
    "  \n",
    "    best_combination = np.argmax(accuracies)\n",
    "    best_num_params = n_pca_all[best_combination] * 2 * n_gmm * 2\n",
    "    print(\"[PCA: %d, GMM: %d components, %s, %d parameters] accuracy: %.4f\" \n",
    "          %(n_pca_all[best_combination], n_gmm_all[best_combination], \n",
    "            cov_tp_all[best_combination], best_num_params, accuracies[best_combination]))\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "P6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RXZNhi0Pl8r"
   },
   "outputs": [],
   "source": [
    "1asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lI03pxDKPl8t"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Number of samples per component\n",
    "\n",
    "# Generate random sample, two components\n",
    "\n",
    "X = train_data\n",
    "\n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = GaussianMixture(n_components=n_components,\n",
    "                                      covariance_type=cv_type)\n",
    "        gmm.fit(X)\n",
    "        bic.append(gmm.bic(X))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "\n",
    "bic = np.array(bic)\n",
    "color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n",
    "                              'darkorange'])\n",
    "clf = best_gmm\n",
    "bars = []\n",
    "\n",
    "# Plot the BIC scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "spl = plt.subplot(2, 1, 1)\n",
    "for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n",
    "    xpos = np.array(n_components_range) + .2 * (i - 2)\n",
    "    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n",
    "                                  (i + 1) * len(n_components_range)],\n",
    "                        width=.2, color=color))\n",
    "plt.xticks(n_components_range)\n",
    "plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n",
    "plt.title('BIC score per model')\n",
    "xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n",
    "    .2 * np.floor(bic.argmin() / len(n_components_range))\n",
    "plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n",
    "spl.set_xlabel('Number of components')\n",
    "spl.legend([b[0] for b in bars], cv_types)\n",
    "\n",
    "# Plot the winner\n",
    "splot = plt.subplot(2, 1, 2)\n",
    "Y_ = clf.predict(X)\n",
    "for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n",
    "                                           color_iter)):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Selected GMM: full model, 2 components')\n",
    "plt.subplots_adjust(hspace=.35, bottom=.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kZ1KoZTPl8u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mefwSLDiPl8p"
   },
   "outputs": [],
   "source": [
    "#using BIc\n",
    "\n",
    "#PCA transformation \n",
    "for n_pca in range(1,10):\n",
    "    pca_X_train = pca_mod.fit_transform(train_data)\n",
    "    pca_X_test = pca_mod.transform(test_data)\n",
    "\n",
    "    #separate training data into two groups \n",
    "    X_train_positive = pca_X_train[train_labels == 1]\n",
    "    X_train_negative = pca_X_train[train_labels == 0]\n",
    "\n",
    "    cov_mat_tp = ['spherical', 'diag', 'tied', 'full']\n",
    "\n",
    "    for i, cov_tp in enumerate(cov_mat_tp):\n",
    "        for n_gmm in range(1, 10):\n",
    "            num_params = (n_pca + n_pca) * n_gmm * 2\n",
    "            if(num_params > 50):\n",
    "                break\n",
    "            \n",
    "            gmm_positive_mod = GaussianMixture(n_components=n_gmm, covariance_type=cov_tp, random_state=12345)\n",
    "            gmm_positive_mod.fit(X_train_positive)\n",
    "\n",
    "            gmm_negative_mod = GaussianMixture(n_components=n_gmm, covariance_type=cov_tp, random_state=12345)\n",
    "            gmm_negative_mod.fit(X_train_negative)\n",
    "\n",
    "            #predict the test data against two different models \n",
    "            y_hat_positive = gmm_positive_mod.score_samples(pca_X_test)\n",
    "            y_hat_negative = gmm_negative_mod.score_samples(pca_X_test)\n",
    "\n",
    "            #compare the probability\n",
    "            higher_prob = y_hat_positive > y_hat_negative\n",
    "            accuracy = np.sum(higher_prob == test_labels)/test_labels.shape[0]\n",
    "\n",
    "            n_pca_all.append(n_pca)\n",
    "            n_gmm_all.append(n_gmm)\n",
    "            accuracies.append(accuracy)\n",
    "            cov_tp_all.append(cov_tp)\n",
    "            \n",
    "            #number of parameters: ((# of mean vector + # of covariance matrix) x # GMM components ) x 2 classes\n",
    "            #print(\"[PCA: %d, GMM: %d components, %9s, %2d parameters] accuracy: %.4f\" \n",
    "            #      %(n_pca, n_gmm, cov_tp, num_params, accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Taeil_Goh_p3_draft.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1j7CnNjMJXwJIkwKIT_2sx4EtaOPPcE_E",
     "timestamp": 1562255632020
    },
    {
     "file_id": "1DBrE0aMfKbzUNBJPBpkyxaTi6OLfaWLK",
     "timestamp": 1562255565172
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
