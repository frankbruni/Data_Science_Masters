{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "# General libraries.\n",
    "import pandas as pd \n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/daphneyang/Desktop/5YMIDS_SP21/w266/266_final/nyt_data_collection/dataset/train_dataset.csv')[:1000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating functions\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_text = []\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        lem_text.append(lemmatizer.lemmatize(w))\n",
    "        lem_text.append(\" \")\n",
    "    return ''.join(lem_text)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Takes Text and does the following\n",
    "    1. Remove Stopwords - remove common stopwords in English\n",
    "    2. Removing Numbers -- may want to revisit this if numbers are important (thinking like covid cases and such)\n",
    "    3. Lemmatizes Text - revert word to its base form (ex. studies, studying to study)\n",
    "     \"\"\"\n",
    "    text = str(text)\n",
    "    if len(text) <1:\n",
    "        # if no available first paragraph\n",
    "        return \" \"\n",
    "    else:\n",
    "        no_numbers = re.sub(r'[0-9]', \"\", text)\n",
    "        no_punct = re.sub(r'[^\\w\\s]', '', no_numbers)\n",
    "    # no_punct = [char for char in no_numbers if char not in string.punctuation]\n",
    "    # no_punct = \" \".join(no_punct)\n",
    "    \n",
    "    # no_punct = \"\".join(no_numbers)\n",
    "    \n",
    "        lower_text = [word.lower() for word in no_punct.split()\n",
    "                  if word not in stopwords.words(\"english\")]\n",
    "        lower_text = ' '.join(lower_text)\n",
    "        lemm_text = lemmatize_text(lower_text)\n",
    "        if len(lemm_text) < 1:\n",
    "        # in empty after regex\n",
    "            return \" \"\n",
    "        else:\n",
    "            return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"There are 4572 students in CS61B and CS1A.\"\n",
    "no_numbers = re.sub(r'[0-9]', \"\", text)\n",
    "no_punct = re.sub(r'[^\\w\\s]', '', no_numbers)\n",
    "lower_text = [word.lower() for word in no_punct.split()\n",
    "                if word not in stopwords.words(\"english\")]\n",
    "lower_text = ' '.join(lower_text)\n",
    "lemm_text = lemmatize_text(lower_text).strip()\n",
    "lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning function\n",
    "df['cleaned_first_paragraph'] = df['first_paragraph'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clean(idx):\n",
    "    print(f\"Row Number: {idx}\\n\")\n",
    "    print(f\"Original text: \\n {df.first_paragraph[idx]} \\n\")\n",
    "    print(f\"Cleaned text: \\n {df.cleaned_first_paragraph[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document Clustering using TFIDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df.cleaned_first_paragraph.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output of first 10 feature names from small training sample\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans_fit = kmeans.fit(X)\n",
    "kmeans_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD = []\n",
    "K = range(1,50)\n",
    "for k in tqdm(K):\n",
    "    km = KMeans(n_clusters = k)\n",
    "    km = km.fit(X)\n",
    "    SSD.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "plt.plot(K, SSD, \"bx-\")\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(\"Sum of Squared Distances\")\n",
    "plt.title('Elbow Method For Optimal Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = [s for s in keywords if any(xs in s for xs in ['Coronavirus'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_vectorizer = TfidfVectorizer()\n",
    "X_keywords = vectorizer.fit_transform(covid_df.cleaned_first_paragraph.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD_keywords = []\n",
    "K_keywords = range(1,100)\n",
    "for k in tqdm(K_keywords):\n",
    "    km = KMeans(n_clusters = k)\n",
    "    km = km.fit(X)\n",
    "    SSD_keywords.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K_keywords, SSD_keywords, \"bx-\")\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(\"Sum of Squared Distances\")\n",
    "plt.title('Elbow Method For Optimal Clusters')"
   ]
  },
  {
   "source": [
    "# Subsetting Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/daphneyang/Desktop/5YMIDS_SP21/w266/266_final/nyt_data_collection/dataset/full_nyt_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_subset(df, keyword, column = \"first_paragraph\"):\n",
    "    df = df.dropna(subset=[column])\n",
    "    subset = df[df[column].str.lower().str.contains(keyword)]\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "for i in range(len(df.keywords)):\n",
    "    cleaned_row = df.keywords[i].replace('[','')\n",
    "    cleaned_row = cleaned_row.replace(']','')\n",
    "    cleaned_row = cleaned_row.replace(\"'\",'')\n",
    "    keywords.extend(cleaned_row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.dropna(subset=['first_paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = df[df['first_paragraph'].str.lower().str.contains('coronavirus')]\n",
    "covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.to_csv('../nyt_data_collection/fp_covid_articles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset(df, \"coronavirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = df[df['keywords'].str.lower().str.contains('coronavirus')]\n",
    "covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.to_csv('../nyt_data_collection/covid_articles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(set(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df = df[df['keywords'].str.lower().str.contains('oil')]\n",
    "oil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df.to_csv('../nyt_data_collection/oil_articles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf_df = df[df['keywords'].str.lower().str.contains('masters golf')]\n",
    "golf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf_df.to_csv('../nyt_data_collection/golf_articles.csv', index = False)"
   ]
  }
 ]
}